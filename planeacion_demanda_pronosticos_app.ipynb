{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38122598-dbed-4824-8661-55e5c5a7c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import streamlit as st\n",
    "from io import BytesIO\n",
    "import requests\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef904f-25c0-4605-92f9-a9db1f8f76d5",
   "metadata": {},
   "source": [
    "# <b><font color=\"navy\">1. Promedio Movil Simple PMS</span></font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b4946-17eb-4752-8722-67a75ca821bc",
   "metadata": {},
   "source": [
    "## 1.1 Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8522a9de-7983-4c14-a877-a6669e382aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_datos_desde_github(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "    return pd.read_csv(BytesIO(response.content), encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd504da7-ff31-40a4-8f41-04239949f300",
   "metadata": {},
   "source": [
    "## 1.2. Pre-procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13edab56-b9cf-4aa5-b855-477fc5f4cefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocesamiento_1(df):\n",
    "    # Asignar formato datetime a columna Fecha\n",
    "    df['FECHA'] = pd.to_datetime(df['FECHA'], format='%d-%m-%y')\n",
    "    \n",
    "    # Establecer FECHA como datetime index\n",
    "    df.set_index('FECHA', inplace=True)\n",
    "    \n",
    "    # Eliminar columna COD_ALMACEN\n",
    "    df = df.drop(columns = 'COD_ALMACEN')\n",
    "    \n",
    "    # Filtrar por fechas posteriores a 2021-01-01\n",
    "    df = df[df.index >= '2021-01-01'].copy()\n",
    "    \n",
    "    # Usar función GROUP BY para agrupar demanda por semana, comenzando los lunes y terminando los domingos\n",
    "    df_sem = df.groupby(['COD_SKU', 'DESC_SKU']).resample('W-SUN').sum(numeric_only=True)\n",
    "    \n",
    "    # Resetear Index para aplanar la tabla\n",
    "    df_sem.reset_index(inplace=True)\n",
    "    \n",
    "    # Seleccionar nombres de SKU unicos\n",
    "    unique_ids = df_sem['COD_SKU'].unique()\n",
    "    \n",
    "    # Colocar semanas como columnas con una tabla dinamica\n",
    "    df_sem_td = df_sem.pivot(index=['COD_SKU', 'DESC_SKU'], columns='FECHA', values='DEMANDA').fillna(0)\n",
    "   \n",
    "    # Seleccionar las columnas que comienzan por '202' (las de demanda)\n",
    "    columnas_dem = df_sem_td.filter(like='202')\n",
    "\n",
    "    # Seleccionar las fechas para posteriormente graficar\n",
    "    indice = columnas_dem.columns\n",
    "    \n",
    "    # Llevar valores de demanda a una lista\n",
    "    series_tiempo = columnas_dem.values.tolist()\n",
    "    \n",
    "    return df_sem_td, series_tiempo, indice, unique_ids\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8f570f-b5e1-451a-a092-14e3e0d4a964",
   "metadata": {},
   "source": [
    "df_sem_td = preprocesamiento_1(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a10df-1fa9-4ff3-9a6a-5bc87ba78652",
   "metadata": {},
   "source": [
    "## 1.3. Funciones para calcular PMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a963290-c418-4d72-8c2b-1278016f77a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_datos_demanda(df_sem_td):\n",
    "    # Seleccionar nombres de SKU unicos\n",
    "    unique_ids = df_sem_td['DESC_SKU'].unique()\n",
    "    \n",
    "    # Seleccionar las columnas que comienzan por '202' (las de demanda)\n",
    "    columnas_dem = df_sem_td.filter(like='202')\n",
    "\n",
    "    # Seleccionar las fechas para posteriormente graficar\n",
    "    indice = columnas_dem.columns\n",
    "    \n",
    "    # Llevar valores de demanda a una lista\n",
    "    series_tiempo = columnas_dem.values.tolist()\n",
    "\n",
    "    return series_tiempo, indice, unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "702a004d-ca46-4473-b743-8defb7a8e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promedio_movil(demanda, extra_periods, n, indice):\n",
    "    \n",
    "    # Determina el numero de datos de la demanda\n",
    "    largo_demanda = len(demanda) \n",
    "    # Adiciona el numero de periodos que queremos pronosticar\n",
    "    demanda = np.append(demanda, [np.nan] * extra_periods)\n",
    "    # Crea un arreglo para posteriormente escribir los pronosticos de largo demanda + los perdiodos extras\n",
    "    forecast = np.full(largo_demanda + extra_periods, np.nan)  \n",
    "    \n",
    "    # Itera entre n y el largo de la serie de demanda\n",
    "    for t in range(n, largo_demanda):\n",
    "        # Promedia los datos de la demanda correspondientes segun n y los datos disponibles\n",
    "        forecast[t] = np.mean(demanda[t-n:t]) \n",
    "    for u in range(1,extra_periods + 1):\n",
    "        # Pronostica el periodo siguiente\n",
    "        forecast[t+u] = np.mean(demanda[t-n+1:t+1]) \n",
    "\n",
    "    # Selecciona la ultima fecha del set de datos\n",
    "    max_fecha = indice[-1]\n",
    "    # Genera nuevas fechas semanales a partir de la ultima fecha y el numero de periodos extra\n",
    "    nuevas_fechas = pd.date_range(start=max_fecha + pd.Timedelta(days=7), periods=extra_periods, freq='W-SUN')\n",
    "    # Combina las fechas actuales con las nuevas\n",
    "    indice = indice.append(nuevas_fechas)\n",
    "    # Crea el data frame con los pronósticos\n",
    "    df = pd.DataFrame.from_dict({'DEMANDA': demanda, 'FORECAST': forecast, 'ERROR': demanda-forecast,}) \n",
    "    # Asigna el index\n",
    "    df.index = indice\n",
    "    \n",
    "    # Regresa df como resultado   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35324c98-2de9-413e-8099-e700310fedd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generacion_mejor_promedio_movil(series_tiempo, extra_periods, n_min, n_max, indice, barra_progreso_pms=None):\n",
    "    \n",
    "    # Crea una lista vacia para acumular el pronostico del periodo siguiente \n",
    "    forecast_siguiente = [] \n",
    "    # Crea una lista vacia para acumular el wmape correspondiente de cada referencia\n",
    "    mejor_error = []\n",
    "    # Crea una lista vacia para acumular el n con menor error\n",
    "    mejor_n = []\n",
    "    # Crea una lista vacia para acumular el n con menor error\n",
    "    rmse_mejor_n = []\n",
    "    # Crea una lista vacia para acumular el mejor df para luego graficar\n",
    "    df_graf = []\n",
    "    # Para calculo del error total\n",
    "    total_error_abs = []\n",
    "    total_demanda = []\n",
    "\n",
    "    total_series = len(series_tiempo)\n",
    "    # Itera por cada una de las series de tiempo\n",
    "    for i, serie in enumerate(series_tiempo): \n",
    "        params = []\n",
    "        KPIs = []\n",
    "        dfs = []\n",
    "        RMSE = []\n",
    "        error_abs = []\n",
    "        suma_demanda = []\n",
    "        \n",
    "        for n in range(n_min,n_max):        \n",
    "            # Aplica funcion de promedio por cada n\n",
    "            df_prom_mov =  promedio_movil(serie, extra_periods=extra_periods, n=n, indice=indice)\n",
    "            # Acumula los parametros\n",
    "            params.append(n)\n",
    "            # Acumula las tablas\n",
    "            dfs.append(df_prom_mov)\n",
    "            # Suma la demanda que aplica para calcular el error\n",
    "            sum_dem = df_prom_mov.loc[df_prom_mov['ERROR'].notnull(), 'DEMANDA'].sum()\n",
    "            sum_len = df_prom_mov.loc[df_prom_mov['ERROR'].notnull(), 'DEMANDA'].count()\n",
    "            sum_error_abs = df_prom_mov['ERROR'].abs().sum()\n",
    "            # Calcula el wmape, devuelve 200% si demanda igual o menor que 0\n",
    "            mae_porc = 2 if sum_dem <= 0 else sum_error_abs / sum_dem\n",
    "            rmse = np.sqrt((df_prom_mov['ERROR']**2).sum() / sum_len)\n",
    "            # Acumula KPI\n",
    "            KPIs.append(mae_porc)\n",
    "            RMSE.append(rmse)\n",
    "            error_abs.append(sum_error_abs)\n",
    "            suma_demanda.append(sum_dem)\n",
    "            \n",
    "        # Seleeciona el minimo de cada n\n",
    "        minimo = np.argmin(KPIs)\n",
    "        # Seleciona el n correspondiente al mejor error\n",
    "        mejor_param_n = params[minimo]\n",
    "        # Selecciona el df con el n correspondiente al menor error\n",
    "        mejor_df = dfs[minimo]\n",
    "        # Selecciona el mae% minimo\n",
    "        mae_porc_minimo = np.min(KPIs)\n",
    "        # Selecciona la suma del error absoluto correspondiente al mejor n \n",
    "        sum_error_abs_min = error_abs[minimo]\n",
    "        # Selecciona la suma de la demanda correspondiente al mejor n \n",
    "        sum_dem_min = suma_demanda[minimo]\n",
    "        # Selecciona el rmse correspondiente al mejor n (no necesariamente es el menor rmse)\n",
    "        rmse_n = RMSE[minimo]\n",
    "        # Selecciona el pronostico de la tabla con mejor wmape\n",
    "        forecast =  dfs[minimo]['FORECAST'].iloc[-1]\n",
    "        # Acumula el pronostico\n",
    "        forecast_siguiente.append(forecast)\n",
    "        # Acumula el wmape\n",
    "        mejor_error.append(mae_porc_minimo)\n",
    "        # Acumula el mejor n con base en el indice +1\n",
    "        mejor_n.append(mejor_param_n)\n",
    "        # Acumula el rmse correspondiente al mejor n\n",
    "        rmse_mejor_n.append(rmse_n)\n",
    "        #Calcula error global\n",
    "        total_error_abs.append(sum_error_abs_min)\n",
    "        total_demanda.append(sum_dem_min)\n",
    "        # Acumula el mejor df por cada referencia para luego graficar\n",
    "        df_graf.append(mejor_df)\n",
    "        \n",
    "        if barra_progreso_pms:\n",
    "            barra_progreso_pms.progress((i + 1) / total_series)\n",
    "    # Calcula el mae% de todos los sku\n",
    "    error_global = np.sum(total_error_abs) / np.sum(total_demanda) if np.sum(total_demanda) != 0 else float('inf')\n",
    "    \n",
    "    return forecast_siguiente, mejor_n, rmse_mejor_n, error_global, df_graf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49128e28-811d-4d75-8208-8e2c6c13e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grafica_interactiva(unique_ids, df_graf):\n",
    "    \n",
    "    fig = make_subplots()\n",
    "    \n",
    "    # Create a plot for each DataFrame in df_graf\n",
    "    for i, df in enumerate(df_graf):\n",
    "        unique_id = unique_ids[i]\n",
    "        visible = True if i == 0 else False  # Set the first element to be visible by default\n",
    "        fig.add_trace(go.Scatter(x=df.index, y=df['DEMANDA'], mode='lines', name=f'Demanda - {unique_id}', line=dict(color='teal'), visible=visible))\n",
    "        fig.add_trace(go.Scatter(x=df.index, y=df['FORECAST'], mode='lines', name=f'Pronóstico - {unique_id}', line=dict(dash='dot', color='maroon'), visible=visible))\n",
    "    \n",
    "    # Create update menus\n",
    "    update_menus = [\n",
    "        {\n",
    "            'buttons': [\n",
    "                {\n",
    "                    'label': unique_ids[i],\n",
    "                    'method': 'update',\n",
    "                    'args': [{'visible': [True if j//2 == i else False for j in range(len(df_graf[:len(unique_ids)]) * 2)]}]\n",
    "                }\n",
    "                for i in range(len(unique_ids))\n",
    "            ],\n",
    "            'direction': 'down',\n",
    "            'showactive': True,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Update the layout with dropdown menu\n",
    "    fig.update_layout(\n",
    "        updatemenus=update_menus,\n",
    "        title='Demanda vs Pronostico',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Values',\n",
    "        template='ggplot2'  # Apply ggplot2 style\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "164ccdf2-7ad3-4b9d-b40c-e47e60192e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entregable_pms(forecast_siguiente, mejor_n, rmse_mejor_n, df_sem_td): \n",
    "    df_return_pms =  pd.DataFrame({'n_OPTIMO':mejor_n,'PRONOSTICO_PMS':forecast_siguiente , 'RMSE_PMS':rmse_mejor_n, \n",
    "                          }, index = df_sem_td.index )\n",
    "    return df_return_pms   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a77c53-374c-4239-b9bd-429e235296bd",
   "metadata": {},
   "source": [
    "# <b><font color=\"navy\">2. Suavización Exponencial SE</span></font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ddf32-83ac-4a6e-b95d-0cafc84621d1",
   "metadata": {},
   "source": [
    "### 2.1. Funcion para calcular SE Simple por cada Sku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee6fec8a-e324-40f7-9a9c-a1fd366cbee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suavizacion_exp_simple(demanda, extra_periods, alfa):\n",
    "\n",
    "    # periodos historicos\n",
    "    largo_demanda = len(demanda)\n",
    "\n",
    "    # Acumular np.nan en el arreglo de demanda para cubrir periodos  futuros\n",
    "    d = np.append(demanda, [np.nan]*extra_periods)\n",
    "\n",
    "    # Arreglo para el pronostico\n",
    "    f = np.full(largo_demanda +  extra_periods, np.nan)\n",
    "    # Inicializar modelo\n",
    "    f[1] = d[0]\n",
    "\n",
    "    #Crear todos los pronosticos t+1 hasta el final de los periodos historicos\n",
    "    for t in range(2, largo_demanda+1):\n",
    "        f[t] = alfa*d[t-1]+(1-alfa)*f[t-1]\n",
    "\n",
    "    # Pronostico para los pediodos extra\n",
    "    for t in range(largo_demanda+1, largo_demanda+extra_periods):\n",
    "        # Actualizar el pronostico con el pronostico anterior\n",
    "        f[t] = f[t-1]\n",
    "\n",
    "    df = pd.DataFrame.from_dict({'DEMANDA':d,'FORECAST':f,'ERROR':d-f})\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c09b1b-b043-4a76-81c4-d84d636d48b1",
   "metadata": {},
   "source": [
    "### 2.2 Función para aplicar mejor alfa a todos los Sku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a9c6978-be1f-423b-8d23-3c6fa991db33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generacion_mejor_suavizacion_exp(series_tiempo, extra_periods, alfa_min, alfa_max, barra_progreso_se=None):\n",
    "    \n",
    "    rango_alfa = np.arange(alfa_min, alfa_max+0.01, 0.01)\n",
    "    # Crea una lista vacia para acumular el pronostico del periodo siguiente \n",
    "    forecast_siguiente_se = [] \n",
    "    # Crea una lista vacia para acumular el wmape correspondiente de cada referencia\n",
    "    mejor_error_se = []\n",
    "    # Crea una lista vacia para acumular el alfa con menor error\n",
    "    mejor_alfa = []\n",
    "    # Crea una lista vacia para acumular el alfa con menor error\n",
    "    rmse_mejor_alfa = []\n",
    "\n",
    "    # Para calculo del error total\n",
    "    total_error_abs = []\n",
    "    total_demanda = []\n",
    "    \n",
    "    total_series = len(series_tiempo)\n",
    "    # Itera por cada una de las series de tiempo\n",
    "    for i, serie in enumerate(series_tiempo):\n",
    "        parametros = []\n",
    "        KPIs = []\n",
    "        dfs_se = []\n",
    "        RMSE_se = []\n",
    "        error_abs = []\n",
    "        suma_demanda = []\n",
    "        \n",
    "        for alfa in rango_alfa:\n",
    "                  \n",
    "            # Aplica funcion de promedio por cada alfa\n",
    "            df =  suavizacion_exp_simple(demanda=serie, extra_periods = extra_periods, alfa = alfa)\n",
    "            # Acumula los parametros\n",
    "            parametros.append(alfa)\n",
    "            # Acumula las tablas\n",
    "            dfs_se.append(df)\n",
    "            # Suma la demanda que aplica para calcular el error\n",
    "            sum_dem = df.loc[df['ERROR'].notnull(), 'DEMANDA'].sum()\n",
    "            sum_len = df.loc[df['ERROR'].notnull(), 'DEMANDA'].count()\n",
    "            sum_error_abs = df['ERROR'].abs().sum()\n",
    "            # Calcula el wmape, devuelve 200% si demanda igual o menor que 0\n",
    "            mae_porc = 2 if sum_dem <= 0 else sum_error_abs / sum_dem\n",
    "            rmse = np.sqrt((df['ERROR']**2).sum() / sum_len)\n",
    "            # Acumula KPI\n",
    "            KPIs.append(mae_porc)\n",
    "            RMSE_se.append(rmse)\n",
    "            error_abs.append(sum_error_abs)\n",
    "            suma_demanda.append(sum_dem)\n",
    "            \n",
    "        # Seleeciona el minimo de cada alfa\n",
    "        minimo = np.argmin(KPIs)\n",
    "        # Selecciona el alfa correspondiente al menor error\n",
    "        mejor_param_alfa = parametros[minimo]\n",
    "        # Selecciona el wmape minimo\n",
    "        mae_porc_minimo = np.min(KPIs)\n",
    "        # Selecciona la suma del error absoluto correspondiente al mejor n \n",
    "        sum_error_abs_min = error_abs[minimo]\n",
    "        # Selecciona la suma de la demanda correspondiente al mejor n \n",
    "        sum_dem_min = suma_demanda[minimo]\n",
    "        # Selecciona el rmse correspondiente al mejor n (no necesariamente es el menor rmse)\n",
    "        rmse_alfa = RMSE_se[minimo]\n",
    "        # Selecciona el pronostico de la tabla con mejor wmape\n",
    "        forecast =  dfs_se[minimo]['FORECAST'].iloc[-1]\n",
    "        # Acumula el pronostico\n",
    "        forecast_siguiente_se.append(forecast)\n",
    "        # Acumula el wmape\n",
    "        mejor_error_se.append(mae_porc_minimo)\n",
    "        # Acumula el mejor n con base en el indice +1\n",
    "        mejor_alfa.append(mejor_param_alfa)\n",
    "        # Acumula el rmse correspondiente al mejor n\n",
    "        rmse_mejor_alfa.append(rmse_alfa)\n",
    "        #Calcula error global\n",
    "        total_error_abs.append(sum_error_abs_min)\n",
    "        total_demanda.append(sum_dem_min)\n",
    "\n",
    "        if barra_progreso_se:\n",
    "            barra_progreso_se.progress((i + 1) / total_series)\n",
    "        \n",
    "    error_global_se = np.sum(total_error_abs) / np.sum(total_demanda) if np.sum(total_demanda) != 0 else float('inf')\n",
    "    \n",
    "    return forecast_siguiente_se, mejor_alfa, rmse_mejor_alfa, error_global_se"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3853397-6a85-4aae-9dcd-abdbdad83f70",
   "metadata": {},
   "source": [
    "## Archivo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64627c85-c73a-4631-b9bc-1c724cc62775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entregable_se(forecast_siguiente_se, mejor_alfa, rmse_mejor_alfa, df_sem_td): \n",
    "    df_return_se =  pd.DataFrame({'alfa_OPTIMO':mejor_alfa,'PRONOSTICO_SE':forecast_siguiente_se , 'RMSE_SE':rmse_mejor_alfa\n",
    "                          }, index = df_sem_td.index )\n",
    "    return df_return_se                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ded6814-786a-42b7-a4e7-6a79419d157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entregable(forecast_siguiente, mejor_n, rmse_mejor_n, forecast_siguiente_se, mejor_alfa, rmse_mejor_alfa, df_sem_td): \n",
    "    df_return =  pd.DataFrame({'n_OPTIMO':mejor_n,'PRONOSTICO_PMS':forecast_siguiente , 'RMSE_PMS':rmse_mejor_n, 'alfa_OPTIMO':mejor_alfa,'PRONOSTICO_SE':forecast_siguiente_se , 'RMSE_SE':rmse_mejor_alfa\n",
    "                          }, index = df_sem_td.index )\n",
    "    return df_return      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ea593-bbbe-4914-99a2-9bb43b6cd422",
   "metadata": {},
   "source": [
    "# <b><font color=\"navy\">3. Ejecución APP</span></font></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "203b7cdb-c570-4e2e-87f6-6a1ed0cbf576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-15 10:14:24.994 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\wilfe\\anaconda3\\envs\\env_may_24\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2024-07-15 10:14:24.994 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Split layout into two columnscd Local_files/Python/Eafit/planeacion_demanda/planeacion_demanda_pronosticos_app.py\n",
    "    st.title(\"App para Cálculo de Pronósticos\")\n",
    "\n",
    "    st.sidebar.title('Flujo de Datos')\n",
    "    seccion = st.sidebar.radio('⬇️ ir a:', ('Carga de datos', 'Forecasting', 'Descargar Resultados'))\n",
    "\n",
    "    if 'df_orig' not in st.session_state:\n",
    "        st.session_state.df_orig = None\n",
    "        \n",
    "    if 'unique_ids' not in st.session_state:\n",
    "        st.session_state.unique_ids = []  # Initialize unique_ids as an empty list\n",
    "\n",
    "    if 'df_graf' not in st.session_state:\n",
    "        st.session_state.df_graf = None  # Initialize df_graf as None\n",
    "        \n",
    "    if seccion == 'Carga de datos':\n",
    "\n",
    "        github_url = 'https://raw.githubusercontent.com/wgutierr/planeacion_demanda/main/dataset/demanda_dia.csv'\n",
    "        \n",
    "        if st.button(\"Cargar datos desde GitHub\"):\n",
    "            st.session_state.df_orig = cargar_datos_desde_github(github_url)          \n",
    "              \n",
    "        if st.session_state.df_orig is not None:\n",
    "            st.success('Archivo Cargado Exitosamente')\n",
    "            col1, buffer, col2 = st.columns([5, 1, 2])\n",
    "            \n",
    "            with col1:\n",
    "                st.write(\"Información Cargada:\")\n",
    "                st.dataframe(st.session_state.df_orig)\n",
    "            with col2:\n",
    "                st.metric(label='Filas', value=len(st.session_state.df_orig))\n",
    "                st.metric(label='Columnas', value=len(st.session_state.df_orig.columns))\n",
    "                \n",
    "            df_sem_td, series_tiempo, indice, unique_ids = preprocesamiento_1(st.session_state.df_orig)\n",
    "            \n",
    "            st.session_state.df_sem_td = df_sem_td\n",
    "            st.session_state.series_tiempo = series_tiempo\n",
    "            st.session_state.indice = indice\n",
    "            st.session_state.unique_ids = unique_ids\n",
    "            \n",
    "            col_1_1, col_1_2 = st.columns([3, 1])\n",
    "            \n",
    "            with col_1_1:\n",
    "                st.write(\"Demanda agrupada por semana por SKU\")\n",
    "                st.dataframe(df_sem_td)\n",
    "            with col_1_2:\n",
    "                st.metric(label='Filas', value=len(df_sem_td))\n",
    "                st.metric(label='Columnas', value=len(df_sem_td.columns))\n",
    "                \n",
    "           \n",
    "            \n",
    "            \n",
    "            # Initialize session state variables if they don't exist\n",
    "            if 'extra_periods' not in st.session_state:\n",
    "                st.session_state.extra_periods = []\n",
    "            if 'forecast_siguiente' not in st.session_state:\n",
    "                st.session_state.forecast_siguiente = []\n",
    "            if 'mejor_n' not in st.session_state:\n",
    "                st.session_state.mejor_n = []\n",
    "            if 'rmse_mejor_n' not in st.session_state:\n",
    "                st.session_state.rmse_mejor_n = []\n",
    "            if 'error_global_pms' not in st.session_state:\n",
    "                st.session_state.error_global_pms = None  # Initialize error_global_pms\n",
    "\n",
    "                \n",
    "        else:\n",
    "            st.error('No se ha cargado el archivo de demanda o no se cargó correctamente. Verifique el formato y vuelva a intentarlo.')\n",
    "        \n",
    "            \n",
    "        \n",
    "    elif seccion == 'Forecasting':\n",
    "        \n",
    "        tabs = st.tabs(['Promedio Movil Simple', 'Suavizacion Exponencial'])\n",
    "    \n",
    "        with tabs[0]:\n",
    "            \n",
    "            st.title(\"Promedio Movil Simple\")\n",
    "            \n",
    "            st.write(\"Por favor completar parámetros para la optimización por promedio móvil simple\")\n",
    "            \n",
    "            extra_periods_pms = st.number_input(\"Períodos adicionales PMS\", min_value=1, step=1)\n",
    "            n_min_pms = st.number_input(\"Valor mínimo de n\", min_value=1, max_value=52, step=1, value=3)\n",
    "            n_max_pms = st.number_input(\"Valor máximo de n\", min_value=1, max_value=53, step=1, value=16)\n",
    "            \n",
    "            if st.button('Generar Mejor PMS'):\n",
    "                with st.spinner('Calculando Pronosticos y Errores en Unidades...'):    \n",
    "                    barra_progreso_pms = st.progress(0)\n",
    "                    \n",
    "                    forecast_siguiente, mejor_n, rmse_mejor_n, error_global, df_graf = generacion_mejor_promedio_movil(\n",
    "                        st.session_state.series_tiempo, extra_periods_pms, n_min_pms, n_max_pms, st.session_state.indice, barra_progreso_pms\n",
    "                    )\n",
    "                    \n",
    "                    barra_progreso_pms.progress(100)\n",
    "                    \n",
    "                    st.session_state.forecast_siguiente = forecast_siguiente\n",
    "                    st.session_state.mejor_n = mejor_n\n",
    "                    st.session_state.rmse_mejor_n = rmse_mejor_n\n",
    "                    st.session_state.extra_periods = extra_periods_pms\n",
    "                    st.session_state.error_global_pms = error_global  # Store error_global for PMS\n",
    "                    st.session_state.df_graf = df_graf\n",
    "                    \n",
    "                    col3, buffer2, col4 = st.columns([4, 1, 2])\n",
    "                    with col3:               \n",
    "                        df_resultado_pms = entregable_pms(\n",
    "                        st.session_state.forecast_siguiente, st.session_state.mejor_n, st.session_state.rmse_mejor_n,\n",
    "                        st.session_state.df_sem_td)\n",
    "                        \n",
    "                        st.dataframe(df_resultado_pms)\n",
    "                        st.session_state.df_resultado_pms = df_resultado_pms\n",
    "                    with col4:\n",
    "                        st.metric(label='MAE% Global PMS', value=\"{:.2%}\".format(error_global), delta = 'en  unidades')\n",
    "                     \n",
    "                if st.session_state.unique_ids and st.session_state.df_graf is not None:\n",
    "                    st.write(st.session_state.unique_ids)\n",
    "                    st.write(st.session_state.df_graf)\n",
    "                    #grafica_interactiva(st.session_state.unique_ids, st.session_state.df_graf)\n",
    "                else:\n",
    "                    st.warning('No se han cargado los datos necesarios para generar la gráfica interactiva.')                    \n",
    "                    \n",
    "                    \n",
    "                  \n",
    "        \n",
    "        with tabs[1]:\n",
    "\n",
    "            st.title(\"Suavización Exponencial\")\n",
    "            \n",
    "            st.write(\"Por favor completar parámetros para la optimización por suavización exponencial\")\n",
    "            extra_periods_se = st.number_input(\"Períodos adicionales SE\", min_value=1, step=1)\n",
    "            alfa_min = st.number_input(\"Valor mínimo de alfa\", min_value=0.01, max_value=0.98, step=0.01, value=0.2)\n",
    "            alfa_max = st.number_input(\"Valor máximo de alfa\", min_value=0.02, max_value=0.99, step=0.01, value=0.6)\n",
    "    \n",
    "            if st.button(\"Calcular SE\"):\n",
    "                with st.spinner('Calculando Pronosticos y Errores en Unidades...'):\n",
    "                    barra_progreso_se = st.progress(0)\n",
    "                    forecast_siguiente_se, mejor_alfa, rmse_mejor_alfa, error_global_se = generacion_mejor_suavizacion_exp(\n",
    "                        st.session_state.series_tiempo, extra_periods_se, alfa_min, alfa_max, barra_progreso_se\n",
    "                    )\n",
    "                    barra_progreso_se.progress(100)\n",
    "                    st.session_state.forecast_siguiente_se = forecast_siguiente_se\n",
    "                    st.session_state.mejor_alfa = mejor_alfa\n",
    "                    st.session_state.rmse_mejor_alfa = rmse_mejor_alfa\n",
    "                    col5, buffer3, col6 = st.columns([4, 1, 2])\n",
    "                    with col5:               \n",
    "                        df_resultado_se = entregable_se(\n",
    "                        st.session_state.forecast_siguiente_se, st.session_state.mejor_alfa, st.session_state.rmse_mejor_alfa,\n",
    "                        st.session_state.df_sem_td \n",
    "                        )\n",
    "                        \n",
    "                        st.dataframe(df_resultado_se)\n",
    "                        st.session_state.df_resultado_se = df_resultado_se\n",
    "                        \n",
    "                    with col6:\n",
    "                        st.metric(label='MAE% Global SE', value=\"{:.2%}\".format(error_global_se))\n",
    "                    # Display PMS error if it was previously calculated            \n",
    "\n",
    "    elif seccion == 'Descargar Resultados':\n",
    "        st.title(\"Descargar Resultados\")\n",
    "        if 'df_resultado_pms' in st.session_state and 'df_resultado_se' in st.session_state:\n",
    "       \n",
    "            \n",
    "            df_resultado_final = entregable(\n",
    "                    st.session_state.forecast_siguiente, st.session_state.mejor_n, st.session_state.rmse_mejor_n,\n",
    "                    st.session_state.forecast_siguiente_se, st.session_state.mejor_alfa, st.session_state.rmse_mejor_alfa,            st.session_state.df_sem_td,\n",
    "                                        )\n",
    "                \n",
    "            st.session_state.df_resultado_final = df_resultado_final\n",
    "        \n",
    "            buffer = BytesIO()\n",
    "            st.session_state.df_resultado_final.to_excel(buffer, index=True)\n",
    "            buffer.seek(0)\n",
    "\n",
    "            st.download_button(\n",
    "                label=\"Descargar resultado final como Excel\",\n",
    "                data=buffer,\n",
    "                file_name='resultado_final.xlsx',\n",
    "                mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'\n",
    "            )\n",
    "        else:\n",
    "            st.write(\"No hay resultados disponibles para descargar\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
